version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.9.1
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181" # Expone el puerto 2181 del contenedor al host, necesario para que otros servicios (como Kafka) se conecten
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000  # Intervalo base de tiempo en milisegundos usado por Zookeeper
    healthcheck:
      test: ["CMD", "bash", "-c", "echo 'ruok' | nc localhost 2181"]
      # Este comando pregunta a Zookeeper "¿estás bien?" (ruok), espera una respuesta. Si responde "imok", está saludable.
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - confluent   # Conecta el contenedor a una red llamada "confluent" para comunicarse con otros servicios (como Kafka)

  broker:
    image: confluentinc/cp-server:7.9.1
    hostname: broker
    container_name: broker
    depends_on:
      zookeeper:  # Asegura que Zookeeper esté iniciado antes de iniciar el broker
        condition: service_healthy  # Espera a que Zookeeper esté saludable
    ports:
      - "9092:9092"  # Expone el puerto 9092 del contenedor al host, necesario para que los clientes se conecten al broker
      - "9101:9101"  # Expone el puerto 9101 del contenedor al host, necesario para que los clientes se conecten al broker
    environment:
      KAFKA_BROKER_ID: 1  # Identificador único del broker en el clúster
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'  # Conexión a Zookeeper
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT  # Mapeo de protocolos de seguridad para los listeners
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092  # Direcciones que se anuncian a los clientes
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter  # Reporter para métricas de Confluent
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1  # Factor de replicación para el topic de offsets (1 para entornos dev)
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0  # Sin retraso inicial para rebalanceo de grupos
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1  # Factor de replicación para el topic de licencias
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1  # Factor de replicación para el topic del balanceador
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1  # Mínimo número de réplicas in-sync para el log de transacciones
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1  # Factor de replicación para el log de transacciones
      KAFKA_JMX_PORT: 9101  # Puerto para monitoreo JMX
      KAFKA_JMX_HOSTNAME: localhost  # Hostname para conexiones JMX
      KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:8081  # URL del registro de esquemas
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:29092  # Servidores bootstrap para el reporter de métricas
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1  # Número de réplicas para el topic de métricas
      CONFLUENT_METRICS_ENABLE: 'false'  # Deshabilita las métricas de Confluent
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'  # ID de cliente para soporte (anónimo)
    networks:
      - confluent
    healthcheck:
      test: ['CMD', 'bash', '-c', 'nc -z localhost 9092']
      interval: 10s
      timeout: 5s
      retries: 5

  schema-registry:
    image: confluentinc/cp-schema-registry:7.9.1  # Actualizada versión para coincidir con Kafka y Zookeeper
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      broker:
        condition: service_healthy  # Espera a que el broker esté saludable antes de iniciar
    ports:
      - "8081:8081"  # Expone el puerto del Schema Registry para acceso externo
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry  # Nombre de host para identificar el servicio
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker:29092'  # Conexión al broker de Kafka
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081  # Configura el listener para aceptar conexiones externas
    networks:
      - confluent
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:8081']  # Verifica que el servicio esté respondiendo
      interval: 30s
      timeout: 10s
      retries: 5

  control-center:
    image: confluentinc/cp-enterprise-control-center:7.9.1  # Actualizada versión para coincidir con Kafka y Zookeeper
    hostname: control-center
    container_name: control-center
    depends_on:
      broker:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    ports:
      - "9021:9021"  # Puerto web UI para acceso al panel de Control Center
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'broker:29092'  # Conexión al broker de Kafka
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'  # Conexión al Schema Registry
      CONTROL_CENTER_REPLICATION_FACTOR: 1  # Factor de replicación para entornos dev
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1  # Número de particiones para tópicos internos
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1  # Particiones para el topic de interceptor de monitoreo
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1  # Factor de replicación para tópicos de métricas
      CONFLUENT_METRICS_ENABLE: 'false'  # Deshabilita las métricas de Confluent
      PORT: 9021  # Puerto interno para Control Center
    networks:
      - confluent
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9021/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  webserver:
    image: apache/airflow:2.11.0-python3.12
    command: webserver
    entrypoint: ['/opt/airflow/scripts/entrypoint.sh']
    depends_on:
      - postgres
    environment:
      - LOAD_EX=n
      - EXECUTOR=Sequential
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__WEBSERVER__SECRET_KEY=this_is_a_very_secured_key
    logging:
      options:
        max-size: 10m
        max-file: "3"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./script/entrypoint.sh:/opt/airflow/scripts/entrypoint.sh
      - ./requirements.txt:/opt/airflow/requirements.txt
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 60s
    networks:
      - confluent


  scheduler:
    image: apache/airflow:2.11.0-python3.12
    hostname: scheduler
    container_name: scheduler
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - ./dags:/opt/airflow/dags
      - ./requirements.txt:/opt/airflow/requirements.txt
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    environment:
      - LOAD_EX=n
      - EXECUTOR=Sequential
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__WEBSERVER__SECRET_KEY=this_is_a_very_secured_key
    command: bash -c "pip install -r ./requirements.txt && airflow db upgrade && airflow scheduler"
    ports:
      - "8793:8793"
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname scheduler"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - confluent

  postgres:
    image: postgres:14.0
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    logging:
      options:
        max-size: 10m
        max-file: "3"
    networks:
      - confluent
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
  
  spark-master:
    # image: bitnami/spark:latest
    build:
      context: .
      dockerfile: Dockerfile-spark
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "9090:8080"
      - "7077:7077"
    networks:
      - confluent
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Si quieres varios workers de Spark, copiar este bloque y cambiar el nombre del servicio
  # spark-worker-1, spark-worker-2, etc.
  spark-worker:
    # image: bitnami/spark:latest
    build:
      context: .
      dockerfile: Dockerfile-spark
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1g
      SPARK_MASTER_URL: spark://spark-master:7077
    networks:
      - confluent

  cassandra_db:
    image: cassandra:latest
    container_name: cassandra
    hostname: cassandra
    ports:
      - "9042:9042"
    environment:
      - MAX_HEAP_SIZE=512M
      - HEAP_NEWSIZE=100M
      - CASSANDRA_USERNAME=cassandra
      - CASSANDRA_PASSWORD=cassandra
    volumes:
      - ./:/home
    networks:
      - confluent
    healthcheck:
      test: ["CMD", "cqlsh", "-e", "describe keyspaces"]
      interval: 10s
      timeout: 5s
      retries: 3

networks:
  confluent: